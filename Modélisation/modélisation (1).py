# -*- coding: utf-8 -*-
"""Modélisation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qm2_fXzn6pubPuSBGjMsVGpx3Z5j9xL5
"""

# @title
from IPython.display import display, HTML

display(HTML("""
<div style="
  margin: 18px 0 26px 0;
  padding: 24px 26px;
  border-radius: 18px;
  background: linear-gradient(135deg, #e8f0fb 0%, #d6e4f5 45%, #c7dbf1 100%);
  border: 1px solid #b6cceb;
  box-shadow: 0 10px 24px rgba(31,119,180,0.18);
  color: #1f77b4;
">

  <div style="text-align:center;">

    <div style="
      font-size: 50px;
      font-weight: 900;
      letter-spacing: -0.6px;
      line-height: 1.1;
      margin: 0;
      color: #1f77b4;
    ">
      Projet de Machine Learning
    </div>

    <div style="
      margin-top: 10px;
      font-size: 30px;
      color: #1e3a5f;
      line-height: 1.4;
      font-weight: 600;
    ">
      Thème 7 — Prévision des coûts médicaux (en $)
      <span style="
        display:inline-block;
        margin-left: 10px;
        padding: 4px 12px;
        border-radius: 999px;
        background: #1f77b4;
        color: #ffffff;
        font-weight: 700;
        font-size:18px;
        vertical-align: middle;
      ">
        Regression
      </span>
    </div>

    <div style="
      margin-top: 16px;
      display:flex;
      justify-content:center;
      gap:10px;
    ">
      <div style="height: 6px; width: 120px; background:#1f77b4; border-radius: 999px;"></div>
      <div style="height: 6px; width: 40px; background:#4fa3d1; border-radius: 999px;"></div>
      <div style="height: 6px; width: 14px; background:#9cc7e6; border-radius: 999px;"></div>
    </div>

    <div style="
      margin-top: 16px;
      display:inline-block;
      padding: 8px 14px;
      border-radius: 14px;
      background: rgba(255,255,255,0.7);
      border: 1px solid #b6cceb;
      font-size: 13px;
      color: #0b1f3a;
      font-weight: 700;
    ">
      Modélisation • Comparaison • Choix optimal
    </div>

  </div>
</div>
"""))

# @title
from IPython.display import display, HTML

display(HTML("""
<div style="
  margin: 10px 0;
  padding: 24px 26px;
  border-radius: 18px;
  background: linear-gradient(135deg, #e8f0fb 0%, #d6e4f5 45%, #c7dbf1 100%);
  border: 1px solid #b6cceb;
  box-shadow: 0 10px 24px rgba(31,119,180,0.12);
  font-family: sans-serif;
  color: #1e3a5f;
">

  <table style="width: 100%; border-collapse: collapse; border: none;">
    <tr>
      <td style="width: 50%; padding-bottom: 15px; vertical-align: top; text-align: left;">
        <span style="font-weight: 800; color: #1f77b4; font-size: 16px; text-transform: uppercase;">Rédigé par :</span>
      </td>
      <td style="width: 50%; padding-bottom: 15px; vertical-align: top; text-align: right;">
        <span style="font-weight: 800; color: #1f77b4; font-size: 16px; text-transform: uppercase;">Sous la supervision de :</span>
      </td>
    </tr>

    <tr>
      <td style="padding-bottom: 20px; vertical-align: top; line-height: 1.6;text-align: left;">
        <div style="font-weight: 600; font-size: 15px;">
          NGUEMFOUO NGOUMTSA Célina<br>
          Sarah-Laure FOGWOUNG DJOUFACK<br>
          RASAMOELINA Nihaviana Albert Paulinah<br>
          Cheick Oumar DIALLO<br>
          Ndeye Ramatoulaye Ndoye FALL
        </div>
      </td>
      <td style="padding-bottom: 20px; vertical-align: middle; text-align: right;">
        <div style="font-weight: 700; font-size: 17px">
          Madame Mously DIAW
        </div>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; font-style: italic; font-size: 14px; color: #506a85; text-align: left;">
        Élèves ingénieurs statisticiens économistes
      </td>
      <td style="padding-top: 10px; font-style: italic; font-size: 14px; color: #506a85; text-align: right;">
        Freelance Senior Data Scientist / ML Engineer
      </td>
    </tr>
  </table>

  <div style="margin-top: 25px; text-align: center;">
    <span style="
      display: inline-block;
      padding: 6px 20px;
      background: rgba(255,255,255,0.5);
      border-radius: 999px;
      font-weight: 900;
      font-size: 16px;
      color: #1f77b4;
    ">
      Année académique 2025/2026
    </span>
  </div>
</div>
"""))

"""<center>

# $\color{#1f77b4}{\mathbf{Préliminaires}}$

</center>

* **Importation des packages**
"""

import warnings
warnings.filterwarnings("ignore")

import os
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from yellowbrick.regressor import ResidualsPlot

import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.stats.diagnostic import het_white, normal_ad
from statsmodels.stats.outliers_influence import variance_inflation_factor

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import (
    OneHotEncoder, StandardScaler, MinMaxScaler,
    RobustScaler, MaxAbsScaler
)

from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.metrics import (
    r2_score, mean_squared_error, mean_absolute_error,
    mean_absolute_percentage_error
)

from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR

from sklearn.ensemble import (RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, VotingRegressor)

from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor

"""* **Configuration des paramètres**"""

Random_state = 43
Test_size = 0.2
f_metrics = "outputs/metrics"
f_coef = "outputs/coef_model"
os.makedirs(f_metrics, exist_ok=True)
os.makedirs(f_coef, exist_ok=True)
numerical_cols = ['age', 'bmi', 'children']
categorical_cols = ['sex', 'smoker', 'region']

"""* **Chargement de la base**"""

url = "https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv"
df = pd.read_csv(url)
data = df

# Supression du doublon
df = df.drop_duplicates().reset_index(drop=True)
# Création de log_charges
df['log_charges'] = np.log1p(df['charges'])

"""* **Découpage**"""

X = df.drop(["charges", "log_charges"], axis=1)
y = df.loc[:, 'charges']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=Test_size, random_state=Random_state)

"""* **Fonction de calcul des métriques**"""

def mape(y_true, y_pred, eps=1e-8):
    y_true = np.asarray(y_true)
    y_pred = np.asarray(y_pred)
    denom = np.maximum(np.abs(y_true), eps)
    return np.mean(np.abs((y_true - y_pred) / denom)) * 100

# Fonction pour charges
def test_model_scalers_metrics(model, X_train, X_test, y_train, y_test):
    results = {}
    for name, scaler in scalers.items():
        steps = []
        if scaler is not None:
            steps.append(('scaler', scaler))
        steps.append(('model', model))

        pipeline = Pipeline(steps)
        pipeline.fit(X_train, y_train)
        y_pred_test = pipeline.predict(X_test)
        y_pred_train = pipeline.predict(X_train)

        results[name] = {
            'best_params': "none",
            'R2_test': r2_score(y_test, y_pred_test),
            'RMSE_test': mean_squared_error(y_test, y_pred_test, squared=False),
            'MSE_test':mean_squared_error(y_test, y_pred_test),
            'MAE_test': mean_absolute_error(y_test, y_pred_test),
            'MAPE_test': mape(y_test, y_pred_test),
            'R2_train': r2_score(y_train, y_pred_train),
            'RMSE_train': mean_squared_error(y_train, y_pred_train, squared=False),
            'MSE_train':mean_squared_error(y_train, y_pred_train),
            'MAE_train': mean_absolute_error(y_train, y_pred_train),
            'MAPE_train': mape(y_train, y_pred_train)
        }

    return results, y_pred_test, y_pred_train

# Fonction pour log_charges
def test_model_scalers_metrics_log(model, X_train, X_test, y_train, y_test):
    results = {}
    y_log_train = np.log1p(y_train)
    y_log_test = np.log1p(y_test)

    for name, scaler in scalers.items():
        steps = []
        if scaler is not None:
            steps.append(('scaler', scaler))
        steps.append(('model', model))

        pipeline = Pipeline(steps)
        pipeline.fit(X_train, y_log_train)

        # Prédictions en log
        y_log_pred_test = pipeline.predict(X_test)
        y_log_pred_train = pipeline.predict(X_train)

        # retransformation des prédictions
        y_pred_test = np.expm1(y_log_pred_test)
        y_pred_train = np.expm1(y_log_pred_train)

        results[name] = {
            'best_params': "none",
            'R2_test': r2_score(y_test, y_pred_test),
            'RMSE_test': mean_squared_error(y_test, y_pred_test, squared=False),
            'MSE_test':mean_squared_error(y_test, y_pred_test),
            'MAE_test': mean_absolute_error(y_test, y_pred_test),
            'MAPE_test': mape(y_test, y_pred_test),
            'R2_train': r2_score(y_train, y_pred_train),
            'RMSE_train': mean_squared_error(y_train, y_pred_train, squared=False),
            'MSE_train':mean_squared_error(y_train, y_pred_train),
            'MAE_train': mean_absolute_error(y_train, y_pred_train),
            'MAPE_train': mape(y_train, y_pred_train)
        }

    return results, y_pred_test, y_pred_train

# Définition de la foonction pour cross validation

def test_models_metrics(model, param_grid, preprocessor, X_train, X_test, y_train, y_test):
    pipeline = Pipeline(steps=[
        ("preprocessor", preprocessor),
        ("model", model)
    ])
    model_cv = GridSearchCV(
        pipeline,
        param_grid,
        cv=5,
        scoring="neg_root_mean_squared_error",
        n_jobs=-1,
        return_train_score=True)

    model_cv.fit(X_train, y_train)
    y_pred_test = model_cv.predict(X_test)
    y_pred_train = model_cv.predict(X_train)

    results = {
        'best_params': model_cv.best_params_,
        'R2_test': r2_score(y_test, y_pred_test),
        'RMSE_test': mean_squared_error(y_test, y_pred_test, squared=False),
        'MSE_test':mean_squared_error(y_test, y_pred_test),
        'MAE_test': mean_absolute_error(y_test, y_pred_test),
        'MAPE_test': mape(y_test, y_pred_test),
        'R2_train': r2_score(y_train, y_pred_train),
        'RMSE_train': mean_squared_error(y_train, y_pred_train, squared=False),
        'MSE_train':mean_squared_error(y_train, y_pred_train),
        'MAE_train': mean_absolute_error(y_train, y_pred_train),
        'MAPE_train': mape(y_train, y_pred_train)
    }

    return results, model_cv


# Définition de la foonction pour cross validation (avec log)
def test_models_metrics_log(model, param_grid, preprocessor, X_train, X_test, y_train, y_test):
    y_log_train = np.log1p(y_train)
    y_log_test = np.log1p(y_test)

    pipeline = Pipeline(steps=[
        ("preprocessor", preprocessor),
        ("model", model)
    ])
    model_cv = GridSearchCV(
        pipeline,
        param_grid,
        cv=5,
        scoring="neg_root_mean_squared_error",
        n_jobs=-1,
        return_train_score=True)

    model_cv.fit(X_train, y_log_train)
    y_log_pred_test = model_cv.predict(X_test)
    y_log_pred_train = model_cv.predict(X_train)

    # retransformation des prédictions
    y_pred_test = np.expm1(y_log_pred_test)
    y_pred_train = np.expm1(y_log_pred_train)


    results = {
        'best_params': model_cv.best_params_,
        'R2_test': r2_score(y_test, y_pred_test),
        'RMSE_test': mean_squared_error(y_test, y_pred_test, squared=False),
        'MSE_test':mean_squared_error(y_test, y_pred_test),
        'MAE_test': mean_absolute_error(y_test, y_pred_test),
        'MAPE_test': mape(y_test, y_pred_test),
        'R2_train': r2_score(y_train, y_pred_train),
        'RMSE_train': mean_squared_error(y_train, y_pred_train, squared=False),
        'MSE_train':mean_squared_error(y_train, y_pred_train),
        'MAE_train': mean_absolute_error(y_train, y_pred_train),
        'MAPE_train': mape(y_train, y_pred_train)
    }

    return results, model_cv

# Fonction pour les modèles ensemblistes
def evaluate_regression(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    return {
        "MAE": mean_absolute_error(y_true, y_pred),
        "MSE": mse,
        "RMSE": np.sqrt(mse),
        "R2": r2_score(y_true, y_pred),
        "MAPE(%)": mape(y_true, y_pred)
    }

"""<center>

# $\color{#1f77b4}{\mathbf{I.\ Modèles\ linéaires}}$

</center>

<center>

## $\color{#1f77b4}{\mathbf{1.\ Régression\ linéaire}}$

</center>
"""

# Pour la regression linéaire, aucun scaler n'est necessaire
scalers = {'none': None}

"""<center>

### $\color{#4fa3d1}{\mathbf{1.1\ Régression\ linéaire\ (sur\ charges)}}$

</center>
"""

model = LinearRegression()
preprocessor = ColumnTransformer(
    transformers=[("Scalers", StandardScaler(), numerical_cols),
                  ("cat_encoding", OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols)])

pipeline = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("model", model)
])

"""<center>

#### $\color{#4fa3d1}{\mathbf{a)\ Exécution\ de\ la\ régression\ linéaire}}$

</center>
"""

pipeline.fit(X_train, y_train)

# Calcul des métriques
linear = test_model_scalers_metrics(pipeline, X_train, X_test, y_train, y_test)

# Résultats
metrics_linear = pd.DataFrame(linear[0]).T
metrics_linear_test = metrics_linear.filter(regex='_test$')
metrics_linear_train = metrics_linear.filter(regex='_train$')

# Métriques sur les données de test
metrics_linear_test.to_csv(os.path.join(f_metrics, "linear_test.csv"), index=False)
metrics_linear_test

# Métriques sur les données d'entrainement
metrics_linear_train.to_csv(os.path.join(f_metrics, "linear_train.csv"), index=False)
metrics_linear_train

feature_names = preprocessor.get_feature_names_out()
print("Intercept:", model.intercept_)
coef_linear=pd.DataFrame({"Variable":feature_names,"Coefficient":model.coef_})
coef_linear.sort_values(by="Coefficient", ascending=False).to_csv(os.path.join(f_coef, "coef_linear.csv"), index=False)
coef_linear.sort_values(by="Coefficient", ascending=False)

"""<center>

#### $\color{#4fa3d1}{\mathbf{b)\ Validation\ du\ modèle}}$

</center>

##### Analyse de régression
"""

# Instanciation:
reg_lin = smf.ols("charges ~ sex + smoker + region + age + bmi + children", data=df)
# Calculs:
res_lin = reg_lin.fit()
res_lin.summary()

""">Le modèle de régression linéaire explique environ 75 % de la variabilité des charges médicales.
> Les variables les plus influentes sont smoker, l’âge et l’indice de masse corporelle.
> Le sexe et certaines régions n’ont pas d’effet statistiquement significatif.
> Malgré une non-normalité des résidus, la taille de l’échantillon garantit la robustesse des estimations.

##### Prédiction
"""

y_test_pred=linear[1]
y_train_pred=linear[2]

fig = make_subplots(rows=1, cols=2, shared_yaxes=False)

for idx, (name, y_true, y_pred) in enumerate([("Train", y_train, y_train_pred),
                                              ("Test", y_test, y_test_pred),
                                             ]):
    col = idx + 1
    fig.add_trace(go.Scatter(x=y_true, y=y_pred, mode="markers", name=name), row=1, col=col)

    fig.add_shape(type="line",
              x0=y_true.min(), x1=y_true.max(),
              y0=y_true.min(), y1=y_true.max(),
              row=1, col=col)

fig.show()

"""##### Analyse des résidus"""

res_viz = ResidualsPlot(pipeline,
                        is_fitted="auto",
                        qqplot=True,
                        hist=False,
                        train_color="blue",
                        test_color="red",
                       )
res_viz.fit(X_train, y_train)
res_viz.score(X_test, y_test)
res_viz.show(clear_figure=True);

"""##### Tests d'hypothèses"""

# La moyenne des residus est:
res_lin.resid.mean()

"""* **Normalité**

Une `p-value < alpha` (5%, en général) pour le test d'Anderson-Darling indique que l'hypothèse nulle de normalité de la distribution des résidus est rejetée.
"""

print("La p-value du test d'Aderson-Darling vaut", normal_ad(res_lin.resid)[1])

"""Ici, l'hypothèse H0 est rejetée puisque la p-value obtenue est inférieure à 5% (ou 1%), les résidus ne suivent pas une distribution Normale.

* **Homoscedasticité**

**Test de White :** L'hypothèse nulle (${H_0}$) est qu'il y a homoscédasticité (les résidus sont dispersés aléatoirement, la variance d'un individu à un autre est constante), et l'hypothèse alternative (${H_1}$) est qu'il y a hétéroscédasticité.
"""

white_test = het_white(res_lin.resid, res_lin.model.exog)
labels_white_test = ["Test Statistic", "p-value", "F-Statistic", "F-Test p-value"]
print(dict(zip(labels_white_test, white_test)))

"""On fixe un seuil alpha de 5 %, la p-value est ici très inférieure au seuil, on ne peut pas accepter l'hypothèse nulle d'homoscédasticité. L'hypothèse d'homoscédasticité de notre régression linéaire est donc considérée comme vérifiée au risque alpha 5%.

* **Multicolinéarité**

La multicolinéarité se produit lorsque les variables indépendantes sont trop fortement corrélées entre elles.

Le facteur d'inflation de la variance **VIF** identifie la corrélation entre les variables indépendantes et la force de cette corrélation. [Si VIF >1 & VIF <5 corrélation modérée, VIF > 5 niveau critique de multicollinéarité.
"""

X_num = X_train[numerical_cols].copy()
X_num_const = sm.add_constant(X_num, has_constant="add")

vif_df = pd.DataFrame({
    "Variable": X_num_const.columns,
    "VIF": [variance_inflation_factor(X_num_const.values, i) for i in range(X_num_const.shape[1])]
})
vif_df = vif_df[vif_df["Variable"] != "const"].sort_values("VIF", ascending=False).reset_index(drop=True)
vif_df

"""<center>

### $\color{#4fa3d1}{\mathbf{1.2\ Régression\ linéaire\ (sur\ log\_charges)}}$

</center>

<center>

#### $\color{#4fa3d1}{\mathbf{a)\ Exécution\ de\ la\ régression\ linéaire}}$

</center>
"""

pipeline.fit(X_train, y_train)

# Calcul des métriques
linear_log = test_model_scalers_metrics_log(pipeline, X_train, X_test, y_train, y_test)
# Résultats
metrics_linear_log = pd.DataFrame(linear_log[0]).T
metrics_linear_log_test = metrics_linear_log.filter(regex='_test$')
metrics_linear_log_train = metrics_linear_log.filter(regex='_train$')

# Métriques sur les données de test
metrics_linear_log_test.to_csv(os.path.join(f_metrics, "linear_log_test.csv"), index=False)
metrics_linear_log_test

# Métriques sur les données d'entrainement
metrics_linear_log_train.to_csv(os.path.join(f_metrics, "linear_log_train.csv"), index=False)
metrics_linear_log_train

feature_names = preprocessor.get_feature_names_out()
print("Intercept:", model.intercept_)
coef_linear_log=pd.DataFrame({"Variable":feature_names,"Coefficient":model.coef_})
coef_linear_log.sort_values(by="Coefficient", ascending=False).to_csv(os.path.join(f_coef, "coef_linear_log.csv"), index=False)
coef_linear_log.sort_values(by="Coefficient", ascending=False)

"""<center>

#### $\color{#4fa3d1}{\mathbf{b)\ Validation\ du\ modèle}}$

</center>

##### Analyse de régression
"""

# Instanciation:
reg_lin = smf.ols("log_charges ~ sex + smoker + region + age + bmi + children", data=df)
# Calculs:
res_lin = reg_lin.fit()
res_lin.summary()

"""Les erreurs calculées dans le tableau ci-haut sont sur log_charges, donc ne sont pas interpretables. On note cependant que les variables la variable la plus influente est smoker..

##### Prédiction
"""

y_test_pred=linear_log[1]
y_train_pred=linear_log[2]
# linear_log retourne des prédictions déjà retransformées

fig = make_subplots(rows=1, cols=2, shared_yaxes=False)

for idx, (name, y_true, y_pred) in enumerate([("Train", y_train, y_train_pred),
                                              ("Test", y_test, y_test_pred),
                                             ]):
    col = idx + 1
    fig.add_trace(go.Scatter(x=y_true, y=y_pred, mode="markers", name=name), row=1, col=col)

    fig.add_shape(type="line",
              x0=y_true.min(), x1=y_true.max(),
              y0=y_true.min(), y1=y_true.max(),
              row=1, col=col)

fig.show()

"""##### Analyse des résidus"""

res_viz = ResidualsPlot(pipeline,
                        is_fitted="auto",
                        qqplot=True,
                        hist=False,
                        train_color="blue",
                        test_color="red",
                       )
res_viz.fit(X_train, np.log1p(y_train))
res_viz.score(X_test, np.log1p(y_test))
res_viz.show(clear_figure=True);

"""##### Tests d'hypothèses"""

# La moyenne des residus est:
res_lin.resid.mean()

"""* **Normalité**"""

print("La p-value du test d'Aderson-Darling vaut", normal_ad(res_lin.resid)[1])

"""Ici, l'hypothèse H0 est rejetée puisque la p-value obtenue est inférieure à 5% (ou 1%), les résidus ne suivent pas une distribution Normale.

* **Homoscedasticité** (test de White)
"""

white_test = het_white(res_lin.resid, res_lin.model.exog)
labels_white_test = ["Test Statistic", "p-value", "F-Statistic", "F-Test p-value"]
print(dict(zip(labels_white_test, white_test)))

"""On fixe un seuil alpha de 5 %, la p-value est ici très inférieure au seuil, on ne peut pas accepter l'hypothèse nulle d'homoscédasticité. L'hypothèse d'homoscédasticité de notre régression linéaire est donc considérée comme vérifiée au risque alpha 5%.

* **Multicolinéarité**

La multicolinéarité reste la même que précedement.

<center>

## $\color{#1f77b4}{\mathbf{2.\ Régression\ Ridge}}$

</center>

**Défintion des paramètres et du modèle**
"""

model = Ridge()
preprocessor = ColumnTransformer(
    transformers=[("Scalers", StandardScaler(), numerical_cols),
                  ("cat_encoding", OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols)])

param_grid = {
    "preprocessor__Scalers": [
        StandardScaler(),
        MinMaxScaler(),
        RobustScaler(),
        MaxAbsScaler(),
        "passthrough"
    ],
    "model__alpha": np.logspace(-3, 2, 50)
}

"""<center>

### $\color{#4fa3d1}{\mathbf{2.1\ Régression\ Ridge\ (sur\ charges)}}$

</center>
"""

ridge = test_models_metrics(model, param_grid, preprocessor, X_train, X_test, y_train, y_test)
results_ridge = ridge[0]
results_ridge

cv=ridge[1]
feature_names = cv.best_estimator_.named_steps['preprocessor'].get_feature_names_out()
coef=pd.DataFrame({"Variable":feature_names,"Coefficient":cv.best_estimator_.named_steps['model'].coef_})
coef.sort_values(by="Coefficient", ascending=False).to_csv(os.path.join(f_coef, "coef_ridge.csv"), index=False)
coef.sort_values(by="Coefficient", ascending=False)

"""<center>

### $\color{#4fa3d1}{\mathbf{2.2\ Régression\ Ridge\ (sur\ log\_charges)}}$

</center>
"""

ridge_log = test_models_metrics_log(model, param_grid, preprocessor, X_train, X_test, y_train, y_test)
results_ridge_log = ridge_log[0]
results_ridge_log

cv_log=ridge_log[1]
feature_names = cv_log.best_estimator_.named_steps['preprocessor'].get_feature_names_out()
coef_log=pd.DataFrame({"Variable":feature_names,"Coefficient":cv_log.best_estimator_.named_steps['model'].coef_})
coef_log.sort_values(by="Coefficient", ascending=False).to_csv(os.path.join(f_coef, "coef_ridge_log.csv"), index=False)
coef_log.sort_values(by="Coefficient", ascending=False)

"""<center>

## $\color{#1f77b4}{\mathbf{3.\ Régression\ Lasso}}$

</center>

**Défintion des paramètres et du modèle**
"""

model=Lasso()
preprocessor = ColumnTransformer(
    transformers=[("Scalers", StandardScaler(), numerical_cols),
                  ("cat_encoding", OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols)])

param_grid = {
    "preprocessor__Scalers": [
        StandardScaler(),
        MinMaxScaler(),
        RobustScaler(),
        "passthrough"
    ],
    "model__alpha": np.logspace(-3, 2, 50)
}

"""<center>

### $\color{#4fa3d1}{\mathbf{3.1\ Régression\ Lasso\ (sur\ charges)}}$

</center>
"""

lasso = test_models_metrics(model, param_grid, preprocessor, X_train, X_test, y_train, y_test)
results_lasso = lasso[0]
results_lasso

cv=lasso[1]
feature_names = cv.best_estimator_.named_steps['preprocessor'].get_feature_names_out()
coef=pd.DataFrame({"Variable":feature_names,"Coefficient":cv.best_estimator_.named_steps['model'].coef_})
coef.sort_values(by="Coefficient", ascending=False).to_csv(os.path.join(f_coef, "coef_lasso.csv"), index=False)
coef.sort_values(by="Coefficient", ascending=False)

"""<center>

### $\color{#4fa3d1}{\mathbf{3.2\ Régression\ Lasso\ (sur\ log\_charges)}}$

</center>
"""

lasso_log = test_models_metrics_log(model, param_grid, preprocessor, X_train, X_test, y_train, y_test)
results_lasso_log = lasso_log[0]
results_lasso_log

cv_log=lasso_log[1]
feature_names = cv_log.best_estimator_.named_steps['preprocessor'].get_feature_names_out()
coef_log=pd.DataFrame({"Variable":feature_names,"Coefficient":cv_log.best_estimator_.named_steps['model'].coef_})
coef_log.sort_values(by="Coefficient", ascending=False).to_csv(os.path.join(f_coef, "coef_lasso_log.csv"), index=False)
coef_log.sort_values(by="Coefficient", ascending=False)

"""<center>

## $\color{#1f77b4}{\mathbf{4.\ Elastic\ net}}$

</center>

**Défintion des paramètres et du modèle**
"""

model=ElasticNet()
preprocessor = ColumnTransformer(
    transformers=[("Scalers", StandardScaler(), numerical_cols),
                  ("cat_encoding", OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols)])

param_grid = {
    "preprocessor__Scalers": [
        StandardScaler(),
        MinMaxScaler(),
        RobustScaler(),
        "passthrough"
    ],
    "model__alpha": np.logspace(-3, 1, 50),
    "model__l1_ratio": [0.1, 0.3, 0.5, 0.7, 0.9, 1.0]
}

"""<center>

### $\color{#4fa3d1}{\mathbf{4.1\ Elastic\ net\ (sur\ charges)}}$

</center>
"""

en = test_models_metrics(model, param_grid, preprocessor, X_train, X_test, y_train, y_test)
results_en = en[0]
results_en

cv=en[1]
feature_names = cv.best_estimator_.named_steps['preprocessor'].get_feature_names_out()
coef=pd.DataFrame({"Variable":feature_names,"Coefficient":cv.best_estimator_.named_steps['model'].coef_})
coef.sort_values(by="Coefficient", ascending=False).to_csv(os.path.join(f_coef, "coef_en.csv"), index=False)
coef.sort_values(by="Coefficient", ascending=False)

"""<center>

### $\color{#4fa3d1}{\mathbf{4.2\ Elastic\ net\ (sur\ log\_charges)}}$

</center>
"""

en_log = test_models_metrics_log(model, param_grid, preprocessor, X_train, X_test, y_train, y_test)
results_en_log = en_log[0]
results_en_log

cv_log=en_log[1]
feature_names = cv_log.best_estimator_.named_steps['preprocessor'].get_feature_names_out()
coef_log=pd.DataFrame({"Variable":feature_names,"Coefficient":cv_log.best_estimator_.named_steps['model'].coef_})
coef_log.sort_values(by="Coefficient", ascending=False).to_csv(os.path.join(f_coef, "coef_en_log.csv"), index=False)
coef_log.sort_values(by="Coefficient", ascending=False)

"""<center>

# $\color{#1f77b4}{\mathbf{II.\ Modèles\ non\ linéaires}}$

</center>

<center>

## $\color{#1f77b4}{\mathbf{1.\ KNN}}$

</center>

**Défintion des paramètres et du modèle**
"""

model=KNeighborsRegressor()
preprocessor = ColumnTransformer(
    transformers=[("Scalers", StandardScaler(), numerical_cols),
                  ("cat_encoding", OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols)])

param_grid = {
    "preprocessor__Scalers": [
        StandardScaler(),
        MinMaxScaler(),
        RobustScaler(),
        "passthrough"
    ],
    "model__n_neighbors": [5, 10, 20, 30],
    "model__weights": ["uniform", "distance"],
    "model__p": [1, 2]
}

"""<center>

### $\color{#4fa3d1}{\mathbf{1.1\ KNN\ (sur\ charges)}}$

</center>
"""

knn = test_models_metrics(model, param_grid, preprocessor, X_train, X_test, y_train, y_test)
results_knn = knn[0]
results_knn

"""<center>

### $\color{#4fa3d1}{\mathbf{1.2\ KNN\ (sur\ log\_charges)}}$

</center>
"""

knn_log = test_models_metrics_log(model, param_grid, preprocessor, X_train, X_test, y_train, y_test)
results_knn_log = knn_log[0]
results_knn_log

"""<center>

## $\color{#1f77b4}{\mathbf{2.\ Arbre\ de\ décision}}$

</center>

**Défintion des paramètres et du modèle**
"""

model=DecisionTreeRegressor(random_state=Random_state)

preprocessor = ColumnTransformer(
    transformers=[("Scalers", StandardScaler(), numerical_cols),
                  ("cat_encoding", OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols)])

param_grid = {
    "preprocessor__Scalers": [
        StandardScaler(),
        MinMaxScaler(),
        RobustScaler(),
        "passthrough"
    ],
    "model__max_depth": [None, 5, 10, 20],
    "model__min_samples_leaf": [1, 5, 10, 20]
}

"""<center>

### $\color{#4fa3d1}{\mathbf{2.1\ Arbre\ de\ décision\ (sur\ charges)}}$

</center>
"""

dtr = test_models_metrics(model, param_grid, preprocessor, X_train, X_test, y_train, y_test)
results_dtr = dtr[0]
results_dtr

"""<center>

### $\color{#4fa3d1}{\mathbf{2.2\ Arbre\ de\ décision\ (sur\ log\_charges)}}$

</center>
"""

dtr_log = test_models_metrics_log(model, param_grid, preprocessor, X_train, X_test, y_train, y_test)
results_dtr_log = dtr_log[0]
results_dtr_log

"""<center>

## $\color{#1f77b4}{\mathbf{3.\ Forêts\ aléatoires}}$

</center>

**Défintion des paramètres et du modèle**
"""

model=RandomForestRegressor(random_state=Random_state, n_jobs=-1)

preprocessor = ColumnTransformer(
    transformers=[("Scalers", StandardScaler(), numerical_cols),
                  ("cat_encoding", OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols)])

param_grid = {
    "preprocessor__Scalers": [
        StandardScaler(),
        MinMaxScaler(),
        RobustScaler(),
        "passthrough"
    ],
    "model__n_estimators": [200, 500],
    "model__max_depth": [None, 10, 20],
    "model__min_samples_leaf": [1, 5, 10]
}

"""<center>

### $\color{#4fa3d1}{\mathbf{3.1\ Forêts\ aléatoires\ (sur\ charges)}}$

</center>
"""

rf = test_models_metrics(model, param_grid, preprocessor, X_train, X_test, y_train, y_test)
results_rf = dtr_log[0]
results_rf

"""<center>

### $\color{#4fa3d1}{\mathbf{3.2\ Forêts\ aléatoires\ (sur\ log\_charges)}}$

</center>
"""

rf_log = test_models_metrics_log(model, param_grid, preprocessor, X_train, X_test, y_train, y_test)
results_rf_log = rf_log[0]
results_rf_log

"""<center>

## $\color{#1f77b4}{\mathbf{4.\ SVR}}$

</center>

**Défintion des paramètres et du modèle**
"""

model=SVR()
preprocessor = ColumnTransformer(
    transformers=[("Scalers", StandardScaler(), numerical_cols),
                  ("cat_encoding", OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols)])

param_grid = {
    "preprocessor__Scalers": [
        StandardScaler(),
        MinMaxScaler(),
        RobustScaler(),
        "passthrough"
    ],
    "model__kernel": ["rbf", "linear"],
    "model__C": [0.1, 1, 10, 100],
    "model__epsilon": [0.01, 0.1, 0.5],
    "model__gamma": ["scale", "auto"]
}

"""<center>

### $\color{#4fa3d1}{\mathbf{4.1\ SVR\ (sur\ charges)}}$

</center>
"""

svr = test_models_metrics(model, param_grid, preprocessor, X_train, X_test, y_train, y_test)
results_svr = svr[0]
results_svr

"""<center>

### $\color{#4fa3d1}{\mathbf{4.2\ SVR\ (sur\ log\_charges)}}$

</center>
"""

svr_log = test_models_metrics(model, param_grid, preprocessor, X_train, X_test, y_train, y_test)
results_svr_log = svr_log[0]
results_svr_log

"""<center>

# $\color{#1f77b4}{\mathbf{Récapitulatif\ des\ modèles\ linéaires\ et\ non\ linéaires}}$

</center>
"""

results_dict = {
    "Ridge": results_ridge,
    "Ridge (log)": results_ridge_log,
    "Lasso": results_lasso,
    "Lasso (log)": results_lasso_log,
    "ElasticNet": results_en,
    "ElasticNet (log)": results_en_log,
    "Decision Tree": results_dtr,
    "Decision Tree (log)": results_dtr_log,
    "Random Forest": results_rf,
    "Random Forest (log)": results_rf_log,
    "SVR": results_svr,
    "SVR (log)": results_svr_log
}

# DataFrame
df_results = (
    pd.DataFrame(results_dict)
    .T
    .reset_index()
    .rename(columns={"index": "Model"})
)
df_results = df_results.sort_values("RMSE_test", ascending=True)
def highlight_best_rmse(row):
    return ['background-color: #4fa3d1' if row.RMSE_test == df_results.RMSE_test.min() else ''
            for _ in row]
df_results.to_csv(os.path.join(f_metrics, "metrics_lin_non_lin.csv"), index=False)
df_results.style.apply(highlight_best_rmse, axis=1)

"""<center>

# $\color{#1f77b4}{\mathbf{III.\ Méthodes\ ensemblistes}}$

</center>

**Défintion des paramètres et des modèles**
"""

# Prétraitement : OHE sur catégorielles + num passthrough
categorical_features = X_train.select_dtypes(include=["object", "category"]).columns.tolist()
numeric_features = [c for c in X_train.columns if c not in categorical_features]

preprocess = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(drop="first", handle_unknown="ignore"), categorical_features),
        ("num", "passthrough", numeric_features),
    ],
    remainder="drop"
)


# CV
CV = 5
kf = KFold(n_splits=CV, shuffle=True, random_state=Random_state)

# Modèles
models = {
    "RandomForest": RandomForestRegressor(random_state=Random_state, n_jobs=-1),
    "GradientBoosting": GradientBoostingRegressor(random_state=Random_state),
    "AdaBoost": AdaBoostRegressor(random_state=Random_state),
    "XGBoost": XGBRegressor(
        objective="reg:squarederror",
        random_state=Random_state,
        n_jobs=-1
    ),
    "LightGBM": LGBMRegressor(random_state=Random_state, n_jobs=-1),
    "CatBoost": CatBoostRegressor(random_state=Random_state, verbose=0, allow_writing_files=False)
}

# Grilles d'hyperparamètres
param_grids = {
    "RandomForest": {
        "model__n_estimators": [100, 200],
        "model__max_depth": [None, 10, 20],
        "model__min_samples_leaf": [1, 2, 5],
    },
    "GradientBoosting": {
        "model__n_estimators": [100, 200],
        "model__learning_rate": [0.1],
        "model__max_depth": [2, 3],
    },
    "AdaBoost": {
        "model__n_estimators": [50, 100],
        "model__learning_rate": [0.1, 1.0],
    },
    "XGBoost": {
        "model__n_estimators": [100, 200],
        "model__learning_rate": [0.03, 0.1],
        "model__max_depth": [3, 5],
        "model__subsample": [0.8, 1.0],
        "model__colsample_bytree": [0.8, 1.0],
    },
    "LightGBM": {
        "model__n_estimators": [100, 200],
        "model__learning_rate": [0.05, 0.1],
        "model__num_leaves": [15, 31, 63],
    },
    "CatBoost": {
        "model__iterations": [200],
        "model__depth": [4, 6],
        "model__learning_rate": [0.03, 0.1],
    }
}

"""<center>

## $\color{#1f77b4}{\mathbf{1.\ Modèles\ ensemblistes\ sur\ charges\ et\ avec\ one\ hot\ encoder}}$

</center>
"""

# Entraînement + évaluation
results = []

for name, model in models.items():
    pipe = Pipeline(steps=[
        ("preprocess", preprocess),
        ("model", model)
    ])

    gs = GridSearchCV(
        estimator=pipe,
        param_grid=param_grids[name],
        cv=kf,
        scoring="neg_root_mean_squared_error",
        n_jobs=-1,
        refit=True
    )

    # Fit uniquement sur TRAIN
    gs.fit(X_train, y_train)

    best_model = gs.best_estimator_
    best_params = gs.best_params_
    best_cv_rmse = -gs.best_score_  # car score négatif

    # Prédictions train/test (utile pour lire l'overfitting)
    y_pred_train = best_model.predict(X_train)
    y_pred_test  = best_model.predict(X_test)

    train_metrics = evaluate_regression(y_train, y_pred_train)
    test_metrics  = evaluate_regression(y_test, y_pred_test)

    results.append({
        "Model": name,
        **{f"{k}_Train": v for k, v in train_metrics.items()},
        **{f"{k}_Test": v for k, v in test_metrics.items()},
        "Best_Params": best_params
    })

summary_ensemble_df = (
    pd.DataFrame(results)
    .sort_values(by="RMSE_Test")
    .reset_index(drop=True)
)

summary_ensemble_df

# TABLEAU FINAL (ordre des colonnes)
cols_final = [
    "Model",
    "RMSE_Train", "MSE_Train", "MAE_Train", "R2_Train", "MAPE(%)_Train",
    "RMSE_Test",  "MSE_Test",  "MAE_Test",  "R2_Test",  "MAPE(%)_Test",
    "Best_Params"
]

summary_ensemble = summary_ensemble_df[cols_final].copy()

summary_ensemble.to_csv(os.path.join(f_metrics, "metrics_ens_ohe.csv"), index=False)
display(summary_ensemble)

"""<center>

## $\color{#1f77b4}{\mathbf{2.\ Modèles\ ensemblistes\ sur\ log\_charges\ et\ avec\ one\ hot\ encoder}}$

</center>
"""

y_train_log = np.log1p(y_train)
results_log = []

for name, model in models.items():
    # On réutilise le preprocess et le param_grids définis plus haut
    pipe = Pipeline(steps=[("preprocess", preprocess), ("model", model)])

    gs = GridSearchCV(
        estimator=pipe,
        param_grid=param_grids[name],
        cv=kf,
        scoring="neg_root_mean_squared_error",
        n_jobs=-1,
        refit=True
    )

    gs.fit(X_train, y_train_log)

    # Prédictions et transformation inverse directe
    y_pred_train_real = np.expm1(gs.best_estimator_.predict(X_train))
    y_pred_test_real  = np.expm1(gs.best_estimator_.predict(X_test))

    # Calcul des métriques avec la fonction evaluate_regression déjà définie
    train_metrics = evaluate_regression(y_train, y_pred_train_real)
    test_metrics  = evaluate_regression(y_test, y_pred_test_real)

    results_log.append({
        "Model": name + " (Log)",
        **{f"{k}_Train": v for k, v in train_metrics.items()},
        **{f"{k}_Test": v for k, v in test_metrics.items()},
        "Best_Params": gs.best_params_
    })

summary_log_df = pd.DataFrame(results_log).sort_values(by="RMSE_Test")
summary_log_df.to_csv(os.path.join(f_metrics, "metrics_ens_ohe_log.csv"), index=False)
display(summary_log_df)

"""<center>

## $\color{#1f77b4}{\mathbf{3.\ Modèles\ ensemblistes\ sur\ charges\ et\ sans\ one\ hot\ encoder}}$

</center>
"""

# Préparation des données spécifiques sans one hot encoder
X_train_nat = X_train.copy()
X_test_nat = X_test.copy()

cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()
for col in cat_cols:
    X_train_nat[col] = X_train_nat[col].astype('category')
    X_test_nat[col] = X_test_nat[col].astype('category')

# Grilles (sans préfixe model__)
param_grids_nat = {
    "LightGBM": {"n_estimators": [100, 200], "learning_rate": [0.05, 0.1], "num_leaves": [31]},
    "CatBoost": {"iterations": [200], "depth": [4, 6], "learning_rate": [0.03, 0.1], "cat_features": [cat_cols]}
}

res = []

for name in ["LightGBM", "CatBoost"]:

    gs = GridSearchCV(models[name], param_grids_nat[name], cv=kf, scoring="neg_root_mean_squared_error", n_jobs=-1)
    gs.fit(X_train_nat, y_train)

    # Prédictions
    y_p_tr = gs.best_estimator_.predict(X_train_nat)
    y_p_te = gs.best_estimator_.predict(X_test_nat)

    # Métriques (via ta fonction déjà définie)
    m_tr = evaluate_regression(y_train, y_p_tr)
    m_te = evaluate_regression(y_test, y_p_te)

    res.append({
        "Model": name + " sans onehot encoder", "MSE_Train": m_tr["MSE"],
        "RMSE_Train": m_tr["RMSE"], "MAE_Train": m_tr["MAE"], "R2_Train": m_tr["R2"], "MAPE(%)_Train": m_tr["MAPE(%)"],
        "RMSE_Test": m_te["RMSE"],"MSE_Test": m_te["MSE"], "MAE_Test": m_te["MAE"], "R2_Test": m_te["R2"], "MAPE(%)_Test": m_te["MAPE(%)"],
        "Best_Params": gs.best_params_
    })

summ= pd.DataFrame(res).sort_values(by="RMSE_Test")
summ.to_csv(os.path.join(f_metrics, "metrics_ens.csv"), index=False)
display(summ)

"""<center>

## $\color{#1f77b4}{\mathbf{4.\ Modèles\ ensemblistes\ sur\ log\_charges\ et\ sans\ one\ hot\ encoder}}$

</center>
"""

resul = []

for name in ["LightGBM", "CatBoost"]:

    gs = GridSearchCV(models[name], param_grids_nat[name], cv=kf, scoring="neg_root_mean_squared_error", n_jobs=-1)
    gs.fit(X_train_nat, y_train_log)

    # Prédictions et transformation inverse (np.expm1)
    y_p_tr_real = np.expm1(gs.best_estimator_.predict(X_train_nat))
    y_p_te_real = np.expm1(gs.best_estimator_.predict(X_test_nat))

    # Métriques sur les valeurs réelles
    m_tr = evaluate_regression(y_train, y_p_tr_real)
    m_te = evaluate_regression(y_test, y_p_te_real)

    resul.append({
        "Model": name + "  Log_sans onehot encoder","MSE_Train": m_tr["MSE"],
        "RMSE_Train": m_tr["RMSE"], "MAE_Train": m_tr["MAE"], "R2_Train": m_tr["R2"], "MAPE(%)_Train": m_tr["MAPE(%)"],
        "RMSE_Test": m_te["RMSE"],"MSE_Test": m_te["MSE"], "MAE_Test": m_te["MAE"], "R2_Test": m_te["R2"], "MAPE(%)_Test": m_te["MAPE(%)"],
        "Best_Params": gs.best_params_
    })

summa = pd.DataFrame(resul).sort_values(by="RMSE_Test")
summa.to_csv(os.path.join(f_metrics, "metrics_ens_log.csv"), index=False)
display(summa)

"""<center>

# $\color{#1f77b4}{\mathbf{Récapitulatif\ des\ modèles\ ensemblistes}}$

</center>
"""

all_results = pd.concat([
    summary_ensemble_df, # Scénario 1 (Normal + OHE)
    summary_log_df,            # Scénario 2 (Log + OHE)
    summ,                # Scénario 3 (Normal + Natif)
    summa                 # Scénario 4 (Log + Natif)
], ignore_index=True)

final_ens = all_results.sort_values(by="RMSE_Test").reset_index(drop=True)
final_ens.to_csv(os.path.join(f_metrics, "metrics_ens_final.csv"), index=False)
display(final_ens)

"""<center>

# $\color{#1f77b4}{\mathbf{Conclusion}}$

</center>

Le meilleur modèle retenu (par rapport au RMSE) est **catboost** avec *one hot encoder et sur la variable charges*. Il a un RMSE de **4721.292238** sur les données de test. Les hyperparamètres retenus par la cross validation pour ce modèle sont : **{'model__depth': 4, 'model__iterations': 200, 'model__learning_rate': 0.03}**
"""

